Alex's comments

- It would be nice to know what pre-training is (page 439), and how it differs from training.

- ReLU is typically better in networks with many layers.

- The idea of the hidden layers doing non-linear transformations to the input so as to be able to do linear separations is (at least for me) very enlightening.
