From Vedran:

- In recent times, there has been remarkable progress in (quantum) machine learning, and specifically in the context of, arguably, data analysis: learning properties of (conditional) probability distributions, as is traditionally done using supervised, unsupervised and related modes of learning. 

- Reinforcement learning is often neglected. I grant that "big data'' applications, arguably, have more immediate value in the modern data-driven world. We are, however, driven by the (further out-of-reach) potential of AGI (artificial general intelligence), that is, human level intelligence. AGI naturally requires the capacities to generalize from examples and to identify structures or rules in data, just as is done in the majority of (Q)ML. However, these specialized aspects clearly do not suffice for AGI. We stick to the viewpoint the missing link between AGI of the future, and data-driven ML of today, can be formulated and investigated in the so called agent-environment paradigm for AI (see e.g. Russell and Norvig textbook on AI), which, as a (from a theory perspective) clean special case has reinforcement learning. We take this paradigm as a means to broach the questions of so-called whole agents (as opposed to specialized agents/devices).

- From my perspective, the key conceptual contribution of the paper is to "quantize" this agent-environment paradigm -- more precisely, to provide one potential method to do so. It is worthwhile to note that, while our method is not the only possible, it does capture the settings of most of QML.
From that point on, we focus on the less explored pure RL aspects, and we pluck the lowest of hanging fruits: identify the obviously impossible things (generic quantum speed up in a classical environment, for instance is impossible), where the main (only) contribution is a comparatively rigorous formalization of the otherwise clean concepts. 

- Once the "no-go's" are identified, we are well justified in relaxing many conditions in a quest for improvements in learning related (thus, not just computational) figures of merit.
If task environments are appropriately "quantized" (which is impossible in most current applications of RL, but bear with me, there will be some saving grace) certain things can be done, and this justifies us to talk in terms of key buzz-words like "quantum enhancement".

- The key technical contribution is to find a formal way to connect simple results from oracular quantum computation (e.g. Grover search) with anything to do with learning (note, in our perspective, it is important to conceptually separate raw search problems and "genuine learning problems", and this is discussed to some extent in the long paper).
This is done by formally characterizing environments where fast searching indeed helps: we, essentially "quantize" the exploration phase of RL [think of it as broad exploration of (behaviour) functions performed to avoid local minima] which in turn provides for a more efficient exploitation phase.

- Another way of viewing the above (not in paper) is to imagine a learning agent which can behave as any out of a parametrized family of learning agents. Each agent in such a family will be optimal for *some* environment (c.f. No Free Lunch Theorems), but in the classical case, finding out which one performs best is not-time efficient - it is better to stick with one agent and train that one. In the quantum case, we can identify indirect properties of the environment (an aspect of property testing) using quantum access faster. This allows us to choose which learning model to apply, which will perform better in a given setting. In some cases, this yields an overall improvement, and "luck favoring environments" are examples where this can be formally proven. 

- I promised to (try to) save (a bit of) grace: one of the visions we have in the group has to do with automated quantum experiments, i.e. nano-scale robots which interact with quantum environments on the quantum level. In such a scenario, quantum control we assume may become possible. Another possibility has to do with so-called model-based agents which use internal models of the environment to perform planning and similar. Since this is internal, this again allows for our results to be applied. This was, to some extent discussed in the long paper.
