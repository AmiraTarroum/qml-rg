{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "===============\n",
    "\n",
    "The Python scientific ecosystem is evolving rapidly and things often break backwards compatibility.  Matplotlib 2.0, Scikit-learn 0.18, and QuTiP 4.0. Import everything *except* Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import qutip\n",
    "%matplotlib inline\n",
    "print(\"Matplotlib:\", matplotlib.__version__,\n",
    "      \"\\nScikit-learn:\", sk.__version__,\n",
    "      \"\\nQuTiP:\", qutip.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the line that starts with %. This is a 'magic command' specific to Jupyter. It ensures that images will be plotted inline, instead of popping up in a window. You can look at all magic commands by entering `%quickref`. Some are useful, although most of them are not. The magic commands are not part of Python, so calling them in a script will throw an error. Keep this in mind when you copy code from a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plotting\n",
    "=====\n",
    "Visual language paradigms...\n",
    "\n",
    "2.1 Matplotlib\n",
    "----------------\n",
    "**2.1.1 The basics**\n",
    "\n",
    "This is the bare minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5, 10)\n",
    "plt.plot(x, x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported the module `matplotlib.plot` as `plt`, and we call a function of it called `plot` to plot the square function. You always plot discrete points: `x` is a numpy array containing ten points as a linear approximation between zero and five. On closer inspection, the curve is not smooth: this is because ten points are not enough for the illusion of smoothness. Let us add some more points, labels for the axes, and a title for the figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5, 100)\n",
    "y = x**2\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('The most exciting function ever, full stop.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order in which you add the decorations to your figure does not matter. The figure is not actually created until you execute the cell. Actually, the execution of the cell just triggers the call of the function `plt.show()`, which instructs Matplotlib to draw the figure *and* display it. In a Python script, you would always call it manually. Let us plot the cube function too, and call `plt.show()` manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5, 100)\n",
    "y1 = x**2\n",
    "y2 = x**3\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference with this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plt.show()` resets all settings, so for the second figure, you must set the axes again.\n",
    "\n",
    "Instead of showing the plot on the screen, you can write them to a file, which will also trigger Matplotlib to draw the figure. If you export it in PDF, it will be as scale-invariant as it can possibly be, and ready for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5, 100)\n",
    "y1 = x**2\n",
    "plt.plot(x, y1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig(\"whatever.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.2 Object-oriented paradigm**\n",
    "\n",
    "The stuff that you see displayed is composed of a hierarchical structure of components. On the top level, it is an instance of the `Figure` class. This is what `plt.plot()` creates for you, with all the other underlying structures within. These include the area where you draw, which is technically called the `Axes` class. You may have more than one `Axes` if you have subplots or embedded plots. `Axes` than have x and y axes, which in turn have a scale, ticks, labels, and so on. If you have a single `Axes` class instantiated, like in the examples below, you can access and change most parts of the hierarchy like you did above with the x and y labels and the figure title. If you want to do anything non-trivial, you have to compose the figure and its components yourself. The examples in this section are mainly from [this tutorial](https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb). Let us instantiate an object of the figure class, get an area to draw on, and plot the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this knowledge, we can do inserts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
    "axes2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # insert axes\n",
    "\n",
    "# main figure\n",
    "axes1.plot(x, y1, 'r')\n",
    "axes1.set_xlabel('x')\n",
    "axes1.set_ylabel('y')\n",
    "axes1.set_title('Square function in red')\n",
    "\n",
    "# insert\n",
    "axes2.plot(x, y2, 'b')\n",
    "axes2.set_xlabel('x')\n",
    "axes2.set_ylabel('y')\n",
    "axes2.set_title('Cube function in blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do aribtrary grids of subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2)\n",
    "y = [y1, y2]\n",
    "labels = [\"Square function\", \"Cube function\"]\n",
    "colors = ['r', 'b']\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(x, y[i], colors[i])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(labels[i])  \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib handles LaTeX reasonably well, just put things between $ signs. For instance, we can a fancy legend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y1, label=r\"$y = x^2$\")\n",
    "ax.plot(x, y2, label=r\"$y = x^3$\")\n",
    "ax.legend(loc=2) # upper left corner\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need the leading `r` in the strings to avoid some nastiness with backslashes.\n",
    "\n",
    "The rest is all about exploring the parameter space. Here we manually create a grid (this is necessary if we mix 2D, 3D or polar coordinates), and plot a bunch of things that Matplotlib can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some new data will be necessary\n",
    "n = np.random.randn(100000)\n",
    "t = np.linspace(0, 2 * np.pi, 100)\n",
    "X, Y = np.meshgrid(t, t)\n",
    "Z = (2.7 - 2 * np.cos(Y) * np.cos(X) - 0.7 * np.cos(np.pi - 2*Y)).T\n",
    "\n",
    "# The actual plot\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "axes = [[],[]]\n",
    "axes[0].append(fig.add_subplot(2, 4, 1))\n",
    "axes[0][0].scatter(x, x + 0.25*np.random.randn(len(x)))\n",
    "axes[0][0].set_title(\"Scatter\")\n",
    "axes[0].append(fig.add_subplot(2, 4, 2))\n",
    "axes[0][1].step(x, y1, lw=2)\n",
    "axes[0][1].set_title(\"Step\")\n",
    "axes[0].append(fig.add_subplot(2, 4, 3))\n",
    "axes[0][2].bar(x, y1, align=\"center\", width=0.5, alpha=0.5)\n",
    "axes[0][2].set_title(\"Bar\")\n",
    "axes[0].append(fig.add_subplot(2, 4, 4))\n",
    "axes[0][3].fill_between(x, y1, y2, color=\"green\", alpha=0.5);\n",
    "axes[0][3].set_title(\"Fill between\");\n",
    "axes[1].append(fig.add_subplot(2, 4, 5))\n",
    "axes[1][0].hist(n, bins=100)\n",
    "axes[1][0].set_title(\"Histogram\")\n",
    "axes[1][0].set_xlim((min(n), max(n)))\n",
    "axes[1].append(fig.add_subplot(2, 4, 6))\n",
    "p = axes[1][1].pcolor(X/(2*np.pi), Y/(2*np.pi), Z, cmap=matplotlib.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max())\n",
    "axes[1][1].set_title(\"Color map\")\n",
    "fig.colorbar(p, ax=axes[1][1])\n",
    "axes[1].append(fig.add_subplot(2, 4, 7, projection='3d'))\n",
    "axes[1][2].plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0, antialiased=False)\n",
    "axes[1][2].set_title(\"Surface plot\")\n",
    "axes[1].append(fig.add_subplot(2, 4, 8, polar=True))\n",
    "axes[1][3].plot(t, t, color='blue', lw=3);\n",
    "axes[1][3].set_title(\"Polar coordinates\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Seaborn\n",
    "-------------\n",
    " - Side effect of importing it.\n",
    " - Examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Add Pandas\n",
    "-------------------------\n",
    "Pandas turns Python into a competitor to R. It allows you to do a wide-scale of statistical operations, but even more importantly, it makes low-level data processing chores easy. For instance, we can load the Iris data set given in CSV format in one single line. This CSV reader is actually much better than you would write with two weeks of effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:17:58.378314",
     "start_time": "2017-01-23T20:17:58.165881"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris = pd.read_csv('Iris.csv', index_col=0)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use seaborn for some basic visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:18:18.624685",
     "start_time": "2017-01-23T20:18:18.336268"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T17:09:41.782760",
     "start_time": "2017-01-23T17:09:41.401724"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=iris, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T17:10:04.650076",
     "start_time": "2017-01-23T17:10:02.483697"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue=\"Species\", size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Modelling\n",
    "============\n",
    "\n",
    "3.1 Machine learning prototypes\n",
    "------------------------------------\n",
    "This section is based on the [examples](https://github.com/rasbt/python-machine-learning-book) in the book [Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning). \n",
    "\n",
    "At the end of the day, most machine learning algorithms will output a function that we can use for any data instance. To characterize the learning algorithm, we can plot the decision boundaries that this function produces. The following helper function plots this decision function along the first two dimensions in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:28:43.015645",
     "start_time": "2017-01-23T20:28:42.989802"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=cmap(idx),\n",
    "                    marker=markers[idx], label=cl)\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        # plot all samples\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c='',\n",
    "                    alpha=1.0,\n",
    "                    linewidths=1,\n",
    "                    marker='o',\n",
    "                    s=55, label='test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reload the Iris data set. In principle, we could convert the Pandas data frame from the previous section, but this data set is actually integrated in Scikit-learn, so we just load that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:25:08.239560",
     "start_time": "2017-01-23T20:25:08.219546"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting, we split the data set in a training and validation part. This is a static random split, not something you would use in 10x random cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:25:09.905829",
     "start_time": "2017-01-23T20:25:09.899867"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize the distribution of the features of the data set. Some kind of normalization or standardization is usually a good idea. Certain learning models work well with data vectors of norm 1, for instance. Here we choose standardization because the physical size parameters of the iris species actually follows a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:25:12.362816",
     "start_time": "2017-01-23T20:25:12.349058"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dumbest model we can possibly train is a neural network of a single neuron, trained by stochastic gradient descent. Even this simple model misses only four instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:26:09.330116",
     "start_time": "2017-01-23T20:26:09.322358"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=1000.0, random_state=0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "plot_decision_regions(X_combined_std, y_combined,\n",
    "                      classifier=lr, test_idx=range(105, 150))\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is linear. \n",
    "\n",
    "Notice the `C` parameter in the instantiation of the logistic regression class. This is not a parameter, but a hyperparameter: the training algorithm (the same stochastic gradient descent as in the perceptron) does not optimize over it. It is our task to find a good value for it. The objective function we are optimizing for logistic regression is\n",
    "\n",
    "$$J(\\mathbf{w}) = \\sum_{i=1}^N\\left[-y_i \\log(\\phi(z_i))-(1-y_i)\\log(1-\\phi(z_i))\\right] + \\frac{1}{2C}||w||^2$$,\n",
    "\n",
    "where $z = w^\\top x$. By increasing the regularization term $\\frac{1}{2C}$, we get a sparser model. In our case, it means that fewer features will be factored in. We can plot this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T20:40:33.791262",
     "start_time": "2017-01-23T20:40:33.157403"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights, params = [], []\n",
    "for c in np.arange(-5, 5):\n",
    "    lr = LogisticRegression(C=10**c, random_state=0)\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10**c)\n",
    "\n",
    "weights = np.array(weights)\n",
    "plt.plot(params, weights[:, 0],\n",
    "         label='petal length')\n",
    "plt.plot(params, weights[:, 1], linestyle='--',\n",
    "         label='petal width')\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regularization term makes machine learning very different from statistics, at least as far as structural risk minimization goes. In general, sparser model will have better generalization properties, that is, they are less prone to overfitting. Since there is no explicit way to optimize over the hyperparameter, you typically do something like grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Quantum simulations\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import a handful of functions from QuTiP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:05:16.442258",
     "start_time": "2017-01-31T16:05:15.950133"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from qutip import destroy, basis, steadystate, expect, mcsolve, mesolve, \\\n",
    "    thermal_dm, plot_fock_distribution, matrix_histogram, hinton, tensor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take an example from the QuTiP documentation (Section 3.6.5). It is a system that reaches a steady state is a harmonic oscillator coupled to a thermal environment. The initial state is the $|10\\rangle$ number state, and it is weakly coupled to a thermal environment characterized by an average particle expectation value of $\\langle n\\rangle = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:05:36.191340",
     "start_time": "2017-01-31T16:05:36.180235"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "a = destroy(N)\n",
    "H = a.dag() * a\n",
    "psi0 = basis(N, 10) # initial state\n",
    "kappa = 0.1 # coupling to oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the collapse operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:05:38.943004",
     "start_time": "2017-01-31T16:05:38.937019"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_th_a = 2 # temperature with average of 2 excitations\n",
    "rate = kappa * (1 + n_th_a)\n",
    "c_op_list = [np.sqrt(rate) * a] # decay operators\n",
    "rate = kappa * n_th_a\n",
    "c_op_list.append(np.sqrt(rate) * a.dag()) # excitation operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the steady state and the particle number in the steady state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:05:41.882398",
     "start_time": "2017-01-31T16:05:41.857458"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_state = steadystate(H, c_op_list)\n",
    "fexpt = expect(a.dag() * a, final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the time evolution over a hundred points with two methods: by the Monte Carlo method and by solving the master equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:06:10.576235",
     "start_time": "2017-01-31T16:06:06.938998"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tlist = np.linspace(0, 50, 100)\n",
    "# monte-carlo\n",
    "mcdata = mcsolve(H, psi0, tlist, c_op_list, [a.dag() * a], ntraj=100)\n",
    "# master eq.\n",
    "medata = mesolve(H, psi0, tlist, c_op_list, [a.dag() * a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:06:19.611440",
     "start_time": "2017-01-31T16:06:19.306189"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(tlist, mcdata.expect[0], tlist, medata.expect[0], lw=2)\n",
    "plt.axhline(y=fexpt, color='r', lw=1.5)\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('Number of excitations', fontsize=14)\n",
    "plt.legend(('Monte Carlo', 'Master Equation', 'Steady State'))\n",
    "plt.title('Decay of Fock state $\\left|10\\\\rangle\\\\right.$' + ' in a thermal environment with $\\langle n\\\\rangle=2$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1** Improve the Monte Carlo simulation to approximate the master equation closer. Typing `mcsolve?` will give you a detailed help on the parametrization of the solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuTiP has built-in functions to work with thermal states. Let us consider a state that is on average occupied by two photons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:07:29.953573",
     "start_time": "2017-01-31T16:07:29.944599"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rho_thermal = thermal_dm(N, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T16:07:33.022367",
     "start_time": "2017-01-31T16:07:32.704204"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12,3))\n",
    "axes[0].matshow(rho_thermal.data.toarray().real)\n",
    "axes[0].set_title(\"Matrix plot\")\n",
    "axes[1].bar(np.arange(0, N)-.5, rho_thermal.diag())\n",
    "axes[1].set_xlim([-.5, N])\n",
    "axes[1].set_title(\"Diagonal\")\n",
    "plot_fock_distribution(rho_thermal, fig=fig, ax=axes[2])\n",
    "axes[2].set_title(\"Fock number distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2** Create and study the maximally mixed state of dimension $N$. Here are three possible ways to do it:\n",
    "1. The Cheap Way: import the function `maximally_mixed_dm` from QuTiP.\n",
    "2. The Way of the Boson: use the previous function `thermal_dm` and increase the average particle number until you converge to the maximally mixed state.\n",
    "3. The Church of Nonlocality Way: trace out half of a maximally entangled state. For qubits, you could create a Bell pair with `(tensor(basis(2, 0), basis(2, 0)) + tensor(basis(2, 1), basis(2, 1))).unit()` and then trace out either party. If you generalize this to $N$ dimensions, you get the solution. Bonus points are awarded for simulating the Unruh effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-31T11:05:42.639370",
     "start_time": "2017-01-31T11:05:42.636000"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Writing good code\n",
    "====================\n",
    "\n",
    "- PEP8\n",
    "\n",
    "- Linting\n",
    "\n",
    "- Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Open science\n",
    "===============\n",
    "\n",
    "- Sharing code under open source licence.\n",
    "\n",
    "- Notebook versus repository.\n",
    "\n",
    "- Where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
