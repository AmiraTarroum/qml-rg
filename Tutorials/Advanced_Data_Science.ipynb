{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T22:24:12.786999Z",
     "start_time": "2017-04-18T00:24:12.207422+02:00"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "try:\n",
    "    from urllib2 import Request, urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scraping\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T22:24:12.877070Z",
     "start_time": "2017-04-18T00:24:12.868742+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file(url, filename=None):\n",
    "    if filename is None:\n",
    "        slash = url.rindex(\"/\")\n",
    "        filename = url[slash+1:]\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            content = file.read()\n",
    "    else:\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        content = urlopen(req).read()\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(content)\n",
    "            file.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T06:49:14.220051Z",
     "start_time": "2017-04-18T08:49:14.215940+02:00"
    }
   },
   "source": [
    "3. Cleaning structured data\n",
    "==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_file(\"https://github.com/JabRef/abbrv.jabref.org/raw/master/journals/journal_abbreviations_webofscience.txt\")\n",
    "abbs = pd.read_csv(\"journal_abbreviations_webofscience.txt\", sep='=',\n",
    "                       comment='#', names=[\"Full Journal Title\", \"Abbreviation\"])\n",
    "abbs = abbs.applymap(lambda x: x.upper().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T22:17:29.200066Z",
     "start_time": "2017-04-18T00:17:28.406354+02:00"
    }
   },
   "outputs": [],
   "source": [
    "get_file(\"https://www.researchgate.net/file.PostFileLoader.html?id=558730995e9d9735688b4631&assetKey=AS%3A273803718922244%401442291301717\",\n",
    "         \"2014_SCI_IF.xlsx\")\n",
    "ifs = pd.read_excel(\"2014_SCI_IF.xlsx\", skiprows=2, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T19:15:00.093938Z",
     "start_time": "2017-04-17T21:15:00.088643+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T19:08:46.632041Z",
     "start_time": "2017-04-17T21:08:46.618086+02:00"
    }
   },
   "outputs": [],
   "source": [
    "ifs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T20:19:47.050404Z",
     "start_time": "2017-04-17T22:19:46.212060+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip = len(ifs) - ifs[\"Journal Impact Factor\"].last_valid_index() + 1\n",
    "ifs = pd.read_excel(\"2014_SCI_IF.xlsx\", skiprows=2, index_col=0,\n",
    "                        skip_footer=skip, parse_cols=\"A,B,E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T20:19:47.125997Z",
     "start_time": "2017-04-17T22:19:47.115934+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifs[\"Full Journal Title\"] = ifs[\"Full Journal Title\"].apply(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T22:18:17.605452Z",
     "start_time": "2017-04-18T00:18:17.350303+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Cleaning and filtering of semi-structured data\n",
    "================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T22:19:07.799408Z",
     "start_time": "2017-04-18T00:19:07.794335+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_result(author):\n",
    "    filename = author + \".html\"\n",
    "    url = \"https://arxiv.org/find/all/1/au:+\" + author\n",
    "    url += \"/0/1/0/all/0/1?per_page=400\"\n",
    "    page = get_file(url, filename)\n",
    "    return BeautifulSoup(page, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us study the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T22:19:43.293048Z",
     "start_time": "2017-04-18T00:19:37.950368+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lewenstein = get_search_result(\"Lewenstein_M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the boring header part, the source code of the search result looks like this:\n",
    "\n",
    "```html\n",
    "<h3>Showing results 1 through 389 (of 389 total) for \n",
    "<a href=\"/find/all/1/au:+Lewenstein_M/0/1/0/all/0/1?skip=0&amp;query_id=ff0631708b5d0dd5\">au:Lewenstein_M</a></h3>\n",
    "<dl>\n",
    "<dt>1.  <span class=\"list-identifier\"><a href=\"/abs/1703.09814\" title=\"Abstract\">arXiv:1703.09814</a> [<a href=\"/pdf/1703.09814\" title=\"Download PDF\">pdf</a>, <a href=\"/ps/1703.09814\" title=\"Download PostScript\">ps</a>, <a href=\"/format/1703.09814\" title=\"Other formats\">other</a>]</span></dt>\n",
    "<dd>\n",
    "<div class=\"meta\">\n",
    "<div class=\"list-title mathjax\">\n",
    "<span class=\"descriptor\">Title:</span> Efficient Determination of Ground States of Infinite Quantum Lattice  Models in Three Dimensions\n",
    "</div>\n",
    "<div class=\"list-authors\">\n",
    "<span class=\"descriptor\">Authors:</span> \n",
    "<a href=\"/find/cond-mat/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>, \n",
    "<a href=\"/find/cond-mat/1/au:+Piga_A/0/1/0/all/0/1\">Angelo Piga</a>, \n",
    "<a href=\"/find/cond-mat/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, \n",
    "<a href=\"/find/cond-mat/1/au:+Su_G/0/1/0/all/0/1\">Gang Su</a>, \n",
    "<a href=\"/find/cond-mat/1/au:+Lewenstein_M/0/1/0/all/0/1\">Maciej Lewenstein</a>\n",
    "</div>\n",
    "<div class=\"list-comments\">\n",
    "<span class=\"descriptor\">Comments:</span> 11 pages, 9 figures\n",
    "</div>\n",
    "<div class=\"list-subjects\">\n",
    "<span class=\"descriptor\">Subjects:</span> <span class=\"primary-subject\">Strongly Correlated Electrons (cond-mat.str-el)</span>; Computational Physics (physics.comp-ph)\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "</dd>\n",
    "```\n",
    "This is the entire first result. This might look intimidating, but as long as you know that in HTML, a mark-up starts with `<whatever>` and ends with `</whatever>`, you will find regular and hierarchical patterns. If you stare hard enough, you will see that the `<dd>` tag contains all the information we want. It does not actually matter what `<dd>` is: we are not writing a browser, we scraping data.\n",
    "\n",
    "As a quick sanity check, we can easily extract the titles, and verify that they match the number of search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T17:50:23.072864Z",
     "start_time": "2017-04-17T19:50:23.027887+02:00"
    }
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "for dd in lewenstein.find_all(\"dd\"):\n",
    "    div = dd.find(\"div\", class_=\"list-title mathjax\")\n",
    "    titles.append(div.get_text().strip()[7:])\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T17:50:36.054755Z",
     "start_time": "2017-04-17T19:50:36.049594+02:00"
    }
   },
   "source": [
    "So far so good. The next problem we face is that not all of these papers belong to Maciej Lewenstein: some impostors have the same abbreviated name M. Lewenstein. They are easy to detect if they uses the non-abbreviated name. Let us run through the page again, noting which subject the the impostors publish in. For this, let us introduce another auxiliary function that extract the short name of the the subject. We also note the primary subject when the abbreviated form of the name appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_punctuation(string):\n",
    "    result = string.replace(\".\", \" \")\n",
    "    return \" \".join(result.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T20:20:37.264066Z",
     "start_time": "2017-04-17T22:20:37.114767+02:00"
    }
   },
   "outputs": [],
   "source": [
    "def extract_subject(long_subject):\n",
    "    start = long_subject.index(\"(\")\n",
    "    return long_subject[start+1:-1]\n",
    "\n",
    "true_lewenstein = [\"Maciej Lewenstein\", \"M Lewenstein\"]\n",
    "impostors = set()\n",
    "primary_subjects = set()\n",
    "for dd in lewenstein.find_all(\"dd\"):\n",
    "    alert = False\n",
    "    div = dd.find(\"div\", class_=\"list-authors\")\n",
    "    subject = extract_subject(dd.find(\"span\", class_ = \"primary-subject\").text)\n",
    "    names = [drop_punctuation(a.text) for a in div.find_all(\"a\")]\n",
    "    for name in names:\n",
    "        if re.search(\"M.* Lewenstein\", name):\n",
    "            if name not in true_lewenstein:\n",
    "                impostors.add(name + \" \" + subject)\n",
    "            elif \"Maciej\" not in name:\n",
    "                primary_subjects.add(subject)\n",
    "print(impostors)\n",
    "print(primary_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is only one person, and we can be reasonably confident that Maciej Lewenstein is unlikely to publish in these subjects. The other good news is that all the short forms of the name belong to physics papers, and not computer science. Armed with this knowledge, we can filter out the correct manuscripts. We need to filter one more thing: we are only interested in papers for which the journal reference is given. Further digging in the HTML code lets us find the correct tag. While we are putting together the correct records, we also normalize his name. We define yet another set of auxiliary functions. We zip the main loop's iterator with the `<dt>` tag, because only this one contains the arXiv ID, from which the year can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:49:18.155502Z",
     "start_time": "2017-04-17T23:49:17.893437+02:00"
    }
   },
   "outputs": [],
   "source": [
    "def extract_journal(journal):\n",
    "    start = journal.index(\" \")\n",
    "    raw = journal[start+1:-1]\n",
    "    m = re.search(\"\\d\", raw)\n",
    "    return drop_punctuation(raw[:m.start()]).strip().upper()\n",
    "\n",
    "\n",
    "def extract_title(title):\n",
    "    start = title.index(\" \")\n",
    "    return title[start+1:-1]\n",
    "\n",
    "\n",
    "def extract_id_and_year(arXiv):\n",
    "    start = arXiv.index(\":\")\n",
    "    if \"/\" in arXiv:\n",
    "        year_index = arXiv.index(\"/\")\n",
    "    else:\n",
    "        year_index = start\n",
    "    year = arXiv[year_index+1:year_index+3]\n",
    "    if year[0] == \"9\":\n",
    "        year = int(\"19\" + year)\n",
    "    else:\n",
    "        year = int(\"20\" + year)\n",
    "    return arXiv[start+1:], year\n",
    "\n",
    "papers = []\n",
    "for dd, dt in zip(lewenstein.find_all(\"dd\"), lewenstein.find_all(\"dt\")):\n",
    "    alert = False\n",
    "    id_, year = extract_id_and_year(dt.find(\"a\", attrs={\"title\": \"Abstract\"}).text)\n",
    "    div = dd.find(\"div\", class_=\"list-authors\")\n",
    "    subject = extract_subject(dd.find(\"span\", class_ = \"primary-subject\").text)\n",
    "    journal = dd.find(\"div\", class_ = \"list-journal-ref\")\n",
    "    if journal:\n",
    "        names = [drop_punctuation(a.text) for a in div.find_all(\"a\")]\n",
    "        for i, name in enumerate(names):\n",
    "            if re.search(\"M.* Lewenstein\", name):\n",
    "                if name not in true_lewenstein:\n",
    "                    break\n",
    "                else:\n",
    "                    names[i] = \"Maciej Lewenstein\"\n",
    "        else:\n",
    "            papers.append([id_, extract_title(dd.find(\"div\", class_ = \"list-title mathjax\").text),\n",
    "                           names, subject, year, extract_journal(journal.text)])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T18:46:53.522823Z",
     "start_time": "2017-04-17T20:46:53.508296+02:00"
    }
   },
   "source": [
    "We would be almost done if journal names were all entered same way. Of course day were not. Let us try to standardize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:49:22.562174Z",
     "start_time": "2017-04-17T23:49:20.960350+02:00"
    }
   },
   "outputs": [],
   "source": [
    "for i, paper in enumerate(papers):\n",
    "    journal = paper[-1]\n",
    "    long_name = abbs[abbs[\"Abbreviation\"] == journal]\n",
    "    if len(long_name) > 0:\n",
    "        papers[i][-1] = long_name[\"Full Journal Title\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be still some rotten apples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:49:24.907701Z",
     "start_time": "2017-04-17T23:49:24.553751+02:00"
    }
   },
   "outputs": [],
   "source": [
    "def find_rotten_apples(paper_list):\n",
    "    rotten_apples = []\n",
    "    for paper in paper_list:\n",
    "        match = ifs[ifs[\"Full Journal Title\"] == paper[-1]]\n",
    "        if len(match) == 0:\n",
    "            rotten_apples.append(paper[-1])\n",
    "    return sorted(rotten_apples)\n",
    "rotten_apples = find_rotten_apples(papers)\n",
    "rotten_apples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you start to feel the pain of being a data scientist. The sloppiness of manual data entering is unbounded. Your duty is to clean up this mess. A quick fix is to tinker with the `drop_punctuation` function to replace the retarded encoding of JRC. Then go through the creation of the papers array and the standardization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T20:20:29.923291Z",
     "start_time": "2017-04-17T22:20:29.916825+02:00"
    }
   },
   "outputs": [],
   "source": [
    "def drop_punctuation(string):\n",
    "    result = string.replace(\".\", \" \")\n",
    "    result = result.replace(\",\", \" \")\n",
    "    result = result.replace(\"(\", \" \")\n",
    "    result = result.replace(\": \", \"-\")\n",
    "    return \" \".join(result.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Cut the rotten apples list in half by defining a replacement dictionary and doing another round of standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:34:51.280881Z",
     "start_time": "2017-04-17T23:34:51.274935+02:00"
    }
   },
   "outputs": [],
   "source": [
    "len(rotten_apples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same drill with our other contender, except that the short version of his name is uniquely his. On the other hand, that single nasty accent introduces N+1 spelling variants, which we should standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:49:30.755597Z",
     "start_time": "2017-04-17T23:49:29.659248+02:00"
    }
   },
   "outputs": [],
   "source": [
    "acin = get_search_result(\"Acin_A\")\n",
    "for dd, dt in zip(acin.find_all(\"dd\"), acin.find_all(\"dt\")):\n",
    "    id_, year = extract_id_and_year(dt.find(\"a\", attrs={\"title\": \"Abstract\"}).text)\n",
    "    div = dd.find(\"div\", class_=\"list-authors\")\n",
    "    subject = extract_subject(dd.find(\"span\", class_ = \"primary-subject\").text)\n",
    "    journal = dd.find(\"div\", class_ = \"list-journal-ref\")\n",
    "    if journal:\n",
    "        names = [drop_punctuation(a.text) for a in div.find_all(\"a\")]\n",
    "        journal = extract_journal(journal.text)\n",
    "        long_name = abbs[abbs[\"Abbreviation\"] == journal]\n",
    "        if len(long_name) > 0:\n",
    "            journal = long_name[\"Full Journal Title\"].values[0]\n",
    "        papers.append([id_, extract_title(dd.find(\"div\", class_ = \"list-title mathjax\").text),\n",
    "                       names, subject, year, journal])\n",
    "for paper in papers:\n",
    "    names = paper[2]\n",
    "    for i, name in enumerate(names):\n",
    "        if re.search(\"A.* Ac.?n\", name):\n",
    "            names[i] = \"Antonio Acín\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:35:05.093999Z",
     "start_time": "2017-04-17T23:35:04.580843+02:00"
    }
   },
   "outputs": [],
   "source": [
    "rotten_apples = find_rotten_apples(papers)\n",
    "rotten_apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:49:34.165878Z",
     "start_time": "2017-04-17T23:49:34.142663+02:00"
    }
   },
   "outputs": [],
   "source": [
    "db = pd.merge(pd.DataFrame(papers, columns=[\"arXiv\", \"Title\", \"Authors\", \"Primary Subject\", \"Year\", \"Full Journal Title\"]),\n",
    "                           ifs, how=\"inner\", on=[\"Full Journal Title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we drop duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:49:36.190645Z",
     "start_time": "2017-04-17T23:49:36.184458+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = db.drop_duplicates(subset=\"arXiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Visual analysis\n",
    "==================\n",
    "\n",
    "Since we only focus on two authors, we can add an additional column to help identifying who it is. We also care about the co-authored papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:50:59.194924Z",
     "start_time": "2017-04-17T23:50:59.071406+02:00"
    }
   },
   "outputs": [],
   "source": [
    "def identify_key_authors(authors):\n",
    "    if \"Maciej Lewenstein\" in authors and \"Antonio Acín\" in authors:\n",
    "        return \"AAML\"\n",
    "    elif \"Maciej Lewenstein\" in authors:\n",
    "        return \"ML\"\n",
    "    else:\n",
    "        return \"AA\"\n",
    "\n",
    "db[\"Group\"] = db[\"Authors\"].apply(lambda x: identify_key_authors(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start plotting distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:51:00.735759Z",
     "start_time": "2017-04-17T23:51:00.330137+02:00"
    }
   },
   "outputs": [],
   "source": [
    "groups = [\"AA\", \"ML\", \"AAML\"]\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "for group in groups:\n",
    "    data = db[db[\"Group\"] == group][\"Journal Impact Factor\"]\n",
    "    sns.distplot(data, kde=False, label=group)\n",
    "ax.legend()\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logarithmic scale makes the raw number of papers appear more balanced, which is fair given the difference in age between the two authors. A single Nature paper makes a great outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:42:37.423536Z",
     "start_time": "2017-04-17T23:42:37.400935+02:00"
    }
   },
   "outputs": [],
   "source": [
    "db[db[\"Journal Impact Factor\"] == db[\"Journal Impact Factor\"].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, Toni has another Nature, but that is not on arXiv yet. Not all of Maciej's paper are on arXiv either, especially not the old ones.\n",
    "\n",
    "We can do the same plots with subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:50:20.696344Z",
     "start_time": "2017-04-17T23:50:20.079655+02:00"
    }
   },
   "outputs": [],
   "source": [
    "subjects = db[\"Primary Subject\"].drop_duplicates()\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "for subject in subjects:\n",
    "    data = db[db[\"Primary Subject\"] == subject][\"Journal Impact Factor\"]\n",
    "    sns.distplot(data, kde=False, label=subject)\n",
    "ax.legend()\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are safe with quant-ph and quantum gases, but stay clear of atom physics. It is amusing to restrict the histogram to Professor Acín's subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:42:45.711125Z",
     "start_time": "2017-04-17T23:42:45.335681+02:00"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1)\n",
    "for subject in subjects:\n",
    "    data = db[(db[\"Primary Subject\"] == subject) & (db[\"Group\"] != \"ML\")]\n",
    "    if len(data) > 1:\n",
    "        sns.distplot(data[\"Journal Impact Factor\"], kde=False, label=subject)\n",
    "ax.legend()\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "His topics are somewhat predictable.\n",
    "\n",
    "Let's add one more column to indicate the number of authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:42:52.070473Z",
     "start_time": "2017-04-17T23:42:51.859924+02:00"
    }
   },
   "outputs": [],
   "source": [
    "db[\"#Authors\"] = db[\"Authors\"].apply(lambda x: len(x))\n",
    "sns.stripplot(x=\"#Authors\", y=\"Journal Impact Factor\", data=db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the length of title? Number of words in title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:42:55.952272Z",
     "start_time": "2017-04-17T23:42:54.970947+02:00"
    }
   },
   "outputs": [],
   "source": [
    "db[\"Length of Title\"] = db[\"Title\"].apply(lambda x: len(x))\n",
    "db[\"Number of Words in Title\"] = db[\"Title\"].apply(lambda x: len(x.split()))\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "sns.stripplot(x=\"Length of Title\", y=\"Journal Impact Factor\", data=db, ax=axes[0])\n",
    "sns.stripplot(x=\"Number of Words in Title\", y=\"Journal Impact Factor\", data=db, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do our authors maintain IF over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:53:09.996865Z",
     "start_time": "2017-04-17T23:53:09.553608+02:00"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(10, 5))\n",
    "data = db[db[\"Group\"] != \"ML\"]\n",
    "sns.stripplot(x=\"Year\", y=\"Journal Impact Factor\", data=data, ax=axes[0])\n",
    "axes[0].set_title(\"AA\")\n",
    "data = db[db[\"Group\"] != \"AA\"].sort_values(by=\"Year\")\n",
    "sns.stripplot(x=\"Year\", y=\"Journal Impact Factor\", data=data, ax=axes[1])\n",
    "axes[1].set_title(\"ML\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything, the IF improved over time. Perhaps joining ICFO had something to do with it.\n",
    "\n",
    "**Completely absurd exercise**. Find out if there is any correlation between the non-date part of the arXiv ID and the Impact Factor. For IDs that contain a \"/\", it is the last three digits. For newer IDs, it is everything after \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**. Extend analysis to the citation numbers of individual papers. Scrape data from Google Scholar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
