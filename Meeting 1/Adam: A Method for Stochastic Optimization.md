Notes on Adam: A Method for Stochastic Optimization
===================================================

This paper introduces a derivative of stochastic gradient descent that is now widely used in training deep learning networks. For instance, it is one of the [available optimizers](https://keras.io/optimizers/) in Keras.
